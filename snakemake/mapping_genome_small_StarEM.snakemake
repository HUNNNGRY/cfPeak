include: 'common.snakemake'
#include: 'peak_common.snakemake'

from collections import OrderedDict

#data_dir = config['data_dir']
output_dir = config['output_dir']
#rna_types = config['rna_types']
#adaptor = config['adaptor']
#min_read_length = config['min_read_length']
genome_dir = config['genome_dir']
#max_read_length = config['max_read_length']
#min_base_quality = config['min_base_quality']
#temp_dir = config['temp_dir']
gn_peak_dir = config['output_dir']+"/call_peak_gbamStarEM"

def get_all_inputs(wildcards):
    available_inputs = dict(
        #unmapped_other=expand('{output_dir}/unmapped/{sample_id}/other.fa.gz',
        #    output_dir=output_dir, sample_id=sample_ids),
        map_genome=expand('{output_dir}/gbamStarEM/{sample_id}/bam/{rna_type}.bam', output_dir=output_dir, sample_id=sample_ids, rna_type=['genome']),
        realigned_reverted_gbam=expand("{output_dir}/gbamStarEM/{sample_id}/{bam_dedup_dir}-EM/{rna_type}/merged.sorted.bam",
            output_dir=output_dir, sample_id=sample_ids, bam_dedup_dir=bam_dedup_dir, rna_type=['genome']), #  'merge11RNA_sort',
        # # gbw_EM_norm= expand(gn_peak_dir+'/tbigwig_RNA_EM_normalized/{sample_id}.transcriptome.bigWig',sample_id=sample_ids),
        # gn_summarize_read_counts=expand('{output_dir}/gbamStarEM_summary/read_counts.txt',output_dir=output_dir),
        # gbigwig=expand('{output_dir}/gbamStarEM_bigwig/{sample_id}.{rna_type}.{strand}.bigWig',
        #     output_dir=output_dir, sample_id=sample_ids, rna_type=['genome'], strand=['+', '-']),
        # gbigwig_normalized=expand('{output_dir}/gbamStarEM_bigwig_normalized/{sample_id}.{rna_type}.{strand}.bigWig',
        #     output_dir=output_dir, sample_id=sample_ids, rna_type=['genome'], strand=['+', '-']),
        
        # blockbuster
        blockbuster_out=expand(gn_peak_dir+'/blockbuster_by_sample/min3/{sample_id}.blockbuster', sample_id=sample_ids),
        blockbuster_raw=expand(gn_peak_dir+'/blockbuster_by_sample/min3/{sample_id}_{peak_type}.bed', sample_id=sample_ids, peak_type=['cluster','block']),
        # blockbuster_block=expand(gn_peak_dir+'/blockbuster/min3_block.bed'),
        # blockbuster_bed=expand(gn_peak_dir+'/blockbuster/blockbuster_min3.blockbuster'),
        # blockbuster_bed2=expand(gn_peak_dir+'/blockbuster/blockbuster_min3_{peak_type}.bed',peak_type=['cluster','block']),
        # blockbuster_gtf=expand(gn_peak_dir+'/blockbuster/blockbuster_min3.gtf'),
        # peak_count_matrix_blockbuster=expand(gn_peak_dir+'/count_matrix/blockbuster_min3.txt')

        # clam
        clam_out=expand(gn_peak_dir+'/clam_by_sample/b{bin_size}_p{pvalue}/{sample_id}.bed',bin_size=config['bin_size'], pvalue=config['call_peak_pvalue_clam'], sample_id=sample_ids),
        peak_clam=expand(gn_peak_dir+'/clam/b{bin_size}_p{pvalue}.bed',bin_size=config['bin_size'], pvalue=config['call_peak_pvalue_clam']),
        # peak_count_matrix_clam=expand(gn_peak_dir+'/count_matrix/clam_b{bin_size}_p{pvalue}.txt',bin_size=config['bin_size'], pvalue=config['call_peak_pvalue_clam'])
    
    )
    enabled_inputs = list(available_inputs.keys())
    inputs = []
    for key, l in available_inputs.items():
        if key in enabled_inputs:
            inputs += l
    return inputs

rule all:
    input:
        get_all_inputs

rule mapping_gn_StarEM:
    input:
        #reads='{output_dir}/unmapped/{sample_id}/clean.fa.gz',
        reads=output_dir + '/trimmed/{sample_id}.fastq.gz',
        index=genome_dir + '/genome_index/star/Genome'
    output:
        # unmapped=output_dir + '/unmapped/{sample_id}/genome.fa.gz',
        bam=output_dir + '/gbamStarEM/{sample_id}/bam/genome.bam',
        # bai=output_dir + '/gbamStarEM/{sample_id}/bam/genome.bam.bai'
    log:
        output_dir + '/gbamStarEM/{sample_id}/log/rule.log'
    threads: config['threads_mapping']
    params:
        STAR="/BioII/lulab_b/baopengfei/gitsoft/STAR-2.5.4a/bin/Linux_x86_64",
        index=genome_dir + '/genome_index/star',
        n=50, # hsa_gn: 50; other shorter: 20
        k=100, # multi-mapped:100; default:10
        output_prefix=output_dir + '/gbamStarEM/{sample_id}/bam/',
        output_star=output_dir + '/gbamStarEM/{sample_id}/bam/Aligned.out.bam'
    shell:
        '''
        {params.STAR}/STAR \
            --genomeDir {params.index} \
            --readFilesIn {input.reads} \
            --alignEndsType EndToEnd \
            --runThreadN {threads} \
            --outFileNamePrefix {params.output_prefix} \
            --outSAMtype BAM Unsorted \
            --outReadsUnmapped Fastx \
            --readFilesCommand pigz -d -c \
            --outFilterMultimapNmax {params.k} \
            --seedPerWindowNmax {params.n} > {log} 2>&1
        samtools sort -@ {threads} -o {output.bam} {params.output_star}
        samtools index -@ {threads} {output.bam}
        '''
        # '''pigz -d -c {input.reads} \
        # | bowtie2 -p {threads} --very-fast --no-unal \
        #     --un-gz {output.unmapped} -x {params.index} - -S - \
        # | samtools view -b -o {output.bam} > {log} 2>&1
        # '''
## test
# /BioII/lulab_b/baopengfei/gitsoft/STAR-2.5.4a/bin/Linux_x86_64/STAR \
#     --genomeDir genome/hg38/genome_index/star \
#     --readFilesIn /BioII/lulab_b/baopengfei/projects/WCHSU-FTC/output/TCGA-COAD_small/trimmed/TCGA-AA-3525-01A-02T-0827-13_mirna_gdc_realn.fastq.gz \
#     --alignEndsType EndToEnd --runThreadN 6 \
#     --outFileNamePrefix /BioII/lulab_b/baopengfei/projects/WCHSU-FTC/output/TCGA-COAD_small/gbamStarEM/TCGA-AA-3525-01A-02T-0827-13_mirna_gdc_realn/ \
#     --outSAMtype BAM Unsorted --outReadsUnmapped Fastx --readFilesCommand pigz -d -c --outFilterMultimapNmax 100 --seedPerWindowNmax 50 \
#     > /BioII/lulab_b/baopengfei/projects/WCHSU-FTC/output/TCGA-COAD_small/gbamStarEM/TCGA-AA-3525-01A-02T-0827-13_mirna_gdc_realn/genome.log 2>&1

rule dedup:
    input:
        bam = output_dir+"/gbamStarEM/{sample_id}/bam/{rna_type}.bam"
    output:
        bam = output_dir+"/gbamStarEM/{sample_id}/bam-deduped/{rna_type}.bam",
        bai = output_dir+"/gbamStarEM/{sample_id}/bam-deduped/{rna_type}.bam.bai",
        # bam = output_dir+"/tbam/{sample_id}/"+bam_dedup_dir+"/{rna_type}.bam",
        # bai = output_dir+"/tbam/{sample_id}/"+bam_dedup_dir+"/{rna_type}.bam.bai",
    params:
        # stat = "{output_dir}/tbam/{sample_id}/bam-deduped/{rna_type}",
        tmpdir = temp_dir
    threads: max(12,int(0.5*config['threads_mapping']))
    wildcard_constraints:
        rna_type='(?!merge).*',
        # sample_id='\w+'
    log:
        output_dir+"/gbamStarEM/{sample_id}/log-dedup/{rna_type}.log"
    conda:
        "./envs/cfpeak.yml"
    shell:
        """
        umi_tools dedup --temp-dir {params.tmpdir}  -I {input.bam} -S {output.bam} > {log} 
        samtools index -@ 4 {output.bam}
        """
#sometimes meet mem error, fail/killed without warning, try add downsampling: --subset=0.2
#consider add "|| cp {input.bam} {output.bam}" to avoid null bam (like long RNA-seq map to miRNA)
#--output-stats={params.stat} seem consumes lots of mem
#for long RNA-seq better not sort by pos and index dedup bam, tbam_to_bed below may need sort by name
#samtools index -@ {threads} {output.bam}





#todo: optimize re-flag bam process
rescue="""
        (echo "Start preprocessor at `date`"; 
        python \
            scripts/clam/preprocessor.py \
            -i {input.bam} \
            -o {params.EM_dir} \
            --read-tagger-method {params.tagger} --max-multihits {params.multihits} ; 
        echo "Start realigner at `date`"; 
        python \
            scripts/clam/realigner.py \
            -i {params.multi} \
            -o {params.EM_dir} \
            --winsize {params.winsize} --max-tags -1 --read-tagger-method {params.tagger}; \
        echo "Start revert to orignal realigned bam at `date`"; 
        (samtools view -H {params.realigned} ; \
        samtools view {params.realigned} | \
        awk 'BEGIN{{FS=OFS="\t"}} \
        {{if ($(NF-6) ~ /^os:Z:/) gsub(/os:Z:/, "", $(NF-6)); if ($(NF-5) ~ /^or:i:/)  gsub(/or:i:/, "", $(NF-5)); if ($(NF-4) ~ /^og:Z:/) gsub(/og:Z:/, "", $(NF-4)); if ($(NF-3) ~ /^oq:Z:/) gsub(/oq:Z:/, "", $(NF-3)); if ($(NF-2) ~ /^oa:Z:/) gsub(/oa:Z:/, "", $(NF-2)); if ($(NF-1) ~ /^ob:i:/) gsub(/ob:i:/, "", $(NF-1)); if ($(NF) ~ /^AS:f:/) gsub(/AS:f:/, "", $(NF)); \
        if ( ($2>=256 && $2<512) || ($2>=768 && $2<1024) || ($2>=1280 && $2<1536) || ($2>=1792 && $2<2048) || ($2>=2304 && $2<2560) || ($2>=2816 && $2<3072) || ($2>=3840) ) newFlag=$2-256; score=100*$(NF); score_field="AS:f:"score; {{ print $1,newFlag,$3,$(NF-5),$5,$(NF-4),$(NF-2),$(NF-1),$9,$(NF-6),$(NF-3),score_field }} }} ' ) | \
        samtools view -b > {output.realigned_reverted_bam}; \
        echo "Start revert to orignal uniq bam at `date`"; 
        (samtools view -H {params.uniq} ; \
        samtools view {params.uniq} | \
        awk 'BEGIN{{FS=OFS="\t"}} \
        {{if ($(NF-5) ~ /^os:Z:/) gsub(/os:Z:/, "", $(NF-5)); if ($(NF-4) ~ /^or:i:/)  gsub(/or:i:/, "", $(NF-4)); if ($(NF-3) ~ /^og:Z:/) gsub(/og:Z:/, "", $(NF-3)); if ($(NF-2) ~ /^oq:Z:/) gsub(/oq:Z:/, "", $(NF-2)); if ($(NF-1) ~ /^oa:Z:/) gsub(/oa:Z:/, "", $(NF-1)); if ($(NF) ~ /^ob:i:/) gsub(/ob:i:/, "", $(NF)); \
        score_field="AS:f:100"; {{ print $1,$2,$3,$(NF-4),$5,$(NF-3),$(NF-1),$(NF),$9,$(NF-5),$(NF-2),score_field}} }} ' ) | \
        samtools view -b > {output.uniq_reverted_bam} ) \
        > {log} 2>&1
        rm {params.multi}* {params.realigned}* 
        """
        
rule bam_rescue_multi_gnStarEM:
    input:
        bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'/{rna_type}.bam' 
    output:
        uniq_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/unique.sorted.bam', 
        allreassign_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/allrealigned.sorted.bam',
        uniq_reverted_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/unique.sorted.revertFullLengthReads.bam', 
        realigned_reverted_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/realigned.sorted.revertFullLengthReads.bam', 
    params:
        uniq = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/unique.sorted.bam', 
        multi = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/multi.sorted.bam', 
        realigned = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/realigned.sorted.bam', 
        EM_dir = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}', 
        multihits = 100,
        tagger = "median",
        winsize = 50, # 50, 2000
        # GATK_path = GATK_path
    threads: max(4,int(config['threads_mapping'])) # 12 
    log:
        output_dir+"/gbamStarEM/{sample_id}/log-EM/{sample_id}/{rna_type}.log"
    run:
        shell(rescue)

sort_EM='''
        samtools sort -@ {threads} -T {params.temp_dir} -o {output.merged_bam} {input.uniq_bam}
        samtools index -@ {threads} {output.merged_bam}
        rm {input.uniq_bam} 
        '''

rule sort_gbamStarEM:
    input:
        uniq_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/unique.sorted.revertFullLengthReads.bam', 
    output:
        merged_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/unique.sorted.revertFullLengthReads.sorted.bam', 
        merged_bai = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/unique.sorted.revertFullLengthReads.sorted.bam.bai', 
    params:
        temp_dir=config['temp_dir']
    threads: config['threads_mapping']
    run:
        shell(sort_EM)

merge_EM='''
        samtools merge -f -@ {threads} {params.tmp_bam} {input.uniq_bam} {input.realigned_reverted_bam}
        samtools sort -@ {threads} -o {output.merged_bam} {params.tmp_bam}
        samtools index -@ {threads} {output.merged_bam}
        rm {params.tmp_bam} {input.realigned_reverted_bam} 
        ln -s {output.merged_bam} {output.link_bam}
        ln -s {output.merged_bai} {output.link_bai}
        '''
rule merge_gbamStarEM:
    input:
        uniq_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/unique.sorted.revertFullLengthReads.sorted.bam', 
        realigned_reverted_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/realigned.sorted.revertFullLengthReads.bam', 
    output:
        merged_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/merged.sorted.bam',
        merged_bai = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/merged.sorted.bam.bai', 
        link_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/{sample_id}.bam', 
        link_bai = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/{sample_id}.bam.bai', 
    threads: config['threads_mapping']
    params: 
        tmp_bam = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/merged.bam', 
    run: 
        shell(merge_EM)





## bam to bed/bg/bw
STARgbamtobed='''samtools view -bh {input} | bedtools bamtobed -split -i stdin | pigz -c -p {threads} > {output} # add splitting for gn STAR
        '''
rule gbamEM_to_bed:
    input:
        output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/merged.sorted.bam',
    output:
        gn_peak_dir+'/gbamStarEM_bed/{sample_id}.{rna_type}.bed.gz'
    threads: config['threads_mapping']
    params: 
        # mapq = config['min_map_quality'],
        # subsample = config['downsample'],# 0.1,
    # wildcard_constraints:
    #     # sample_id='\w+'
    run:
        shell(STARgbamtobed)

rule gbam_to_bedgraph:
    input:
        output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/merged.sorted.bam'
    output:
        output_dir+'/gbamStarEM_bedgraph/{sample_id}.{rna_type}.{strand}.bedGraph'
    params:
        temp_dir=config['temp_dir']
    shell:
        '''bedtools genomecov -ibam {input} -strand {wildcards.strand} -bg -split \
            | LC_COLLATE=C sort -T {params.temp_dir} -k1,1 -k2,2n > {output}
        '''

rule gbedgraph_to_bigwig:
    input:
        bedgraph=output_dir+'/gbamStarEM_bedgraph/{sample_id}.{rna_type}.{strand}.bedGraph',
        chrom_sizes=genome_dir + '/chrom_sizes/genome'
    output:
        output_dir+'/gbamStarEM_bigwig/{sample_id}.{rna_type}.{strand}.bigWig'
    shell:
        '''{bin_dir}/bedGraphToBigWig {input.bedgraph} {input.chrom_sizes} {output}
        '''

rule read_counts_mapped_gn:
    input: 
        output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/merged.sorted.bam'
    output:
        output_dir+'/gbamStarEM_stats/read_counts_mapped/{sample_id}/{rna_type}'
    # conda:
    #     "./envs/cfpeak.yml"
    # wildcard_constraints:
    #     rna_type='(?!promoter$)(?!enhancer$)(?!intron$)(?!repeats$)(?!genome).*'
    shell:
        '''bamtools count -in {input} > {output}
        '''

rule summarize_read_counts_gn:
    input:
        # output_dir+'/stats/read_counts_mapped/{sample_id}/{merge_type}' #merge19_sort merge11RNA_sort
        mapped=lambda wildcards: expand(output_dir+'/gbamStarEM_stats/read_counts_mapped/{sample_id}/{rna_type}',
            sample_id=sample_ids, rna_type='genome'), # + ['other', 'promoter', 'enhancer', 'intron', 'repeats']),
    output:
        output_dir+'/gbamStarEM_summary/read_counts.txt'
    run:
        import pandas as pd
        import os
        from collections import OrderedDict
    
        records = OrderedDict()
        for sample_id in sample_ids:
            records[sample_id] = {}
        for filename in input.mapped:
            sample_id, rna_type = filename.split(os.path.sep)[-2:]
            with open(filename, 'r') as f:
                records[sample_id][rna_type + '.mapped'] = int(f.read().strip())
        # for filename in input.unmapped:
        #     sample_id, rna_type = filename.split(os.path.sep)[-2:]
        #     with open(filename, 'r') as f:
        #         records[sample_id][rna_type + '.unmapped'] = int(f.read().strip())
        records = pd.DataFrame.from_records(records)
        records.columns.name = 'sample_id'
        records.columns.name = 'item'
        records.index.name = 'reads_type'
        records.to_csv(output[0], sep='\t', header=True, index=True)

rule normalize_gbigwig:
    input:
        summary=output_dir+'/gbamStarEM_summary/read_counts.txt',
        # bam='{output_dir}/gbamStarEM/{sample_id}/{rna_type}.bam',
        bam=output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/{rna_type}/merged.sorted.bam',
        bigwig=output_dir+'/gbamStarEM_bigwig/{sample_id}.{rna_type}.{strand}.bigWig',
        chrom_sizes=genome_dir + '/chrom_sizes/genome'
    output:
        bigwig=output_dir+'/gbamStarEM_bigwig_normalized/{sample_id}.{rna_type}.{strand}.bigWig',
        bedgraph=temp(output_dir+'/gbamStarEM_bigwig_normalized/{sample_id}.{rna_type}.{strand}.bedGraph')
    wildcard_constraints:
        strand='[+-]'
    run:
        library_size = get_library_size_small(input.summary, wildcards.sample_id, 'genome')
        shell(r'''{bin_dir}/bigWigToBedGraph {input.bigwig} stdout \
            | awk -v d="{library_size}" 'BEGIN{{OFS="\t";FS="\t"}}{{print $1,$2,$3,1000000.0*$4/d}}' > {output.bedgraph}''')
        shell(r'''{bin_dir}/bedGraphToBigWig {output.bedgraph} {input.chrom_sizes} {output.bigwig}''')

# rule normalize_log2_gbigwig:
#     input:
#         bigwig='{output_dir}/bigwig_normalized/{sample_id}.{rna_type}.{strand}.bigWig',
#         chrom_sizes=genome_dir + '/chrom_sizes/genome'
#     output:
#         bigwig='{output_dir}/bigwig_normalized_log2/{sample_id}.{rna_type}.{strand}.bigWig',
#         bedgraph=temp('{output_dir}/bigwig_normalized_log2/{sample_id}.{rna_type}.{strand}.bedGraph')
#     wildcard_constraints:
#         strand='[+-]'
#     shell:
#         r'''bigWigToBedGraph {input.bigwig} stdout \
#             | awk 'BEGIN{{OFS="\t";FS="\t"}}{{print $1,$2,$3,log($4+0.25)/log(2)}}' > {output.bedgraph}
#         bedGraphToBigWig {output.bedgraph} {input.chrom_sizes} {output.bigwig} 
#         '''






peak_recurrence_gn='''cat {input.bed} \
            | bedtools sort \
            | bedtools genomecov -i - -strand + -g {input.chrom_sizes} -bg \
            | awk -v s="+" 'BEGIN{{OFS="\t";FS="\t"}}{{print $1,$2,$3,"X",$4,s}}' \
            > {output.pos}
            cat {input.bed} \
            | bedtools sort \
            | bedtools genomecov -i - -strand - -g {input.chrom_sizes} -bg \
            | awk -v s="-" 'BEGIN{{OFS="\t";FS="\t"}}{{print $1,$2,$3,"X",$4,s}}' \
            > {output.neg}
            cat {output.pos} {output.neg} | LC_COLLATE=C sort -k1,1 -k2,2n > {output.bed}
        '''

filter_peak_recurrence='''awk -v c={params.cov_num} '$5 >= c' {input} \
            | bedtools merge -s -c 2,3,5,6 -o collapse,collapse,collapse,collapse \
            | awk 'BEGIN{{OFS="\t";FS="\t"}} 
            {{split($4,a,/,/); split($5,b,/,/); split($6,c,/,/); split($7,d,/,/);
            cov=0.0;for(i=1;i<=length(a);i++){{cov+=c[i]*(b[i]-a[i]);}} 
            cov /= $3-$2;
            print $1,$2,$3,"peak_" NR,cov,d[1]
            }}' > {output}
        '''





########################################
# run clam
########################################
rule call_peaks_clam:
    input:
        uniq = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/genome/unique.sorted.bam',
        realign = output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/genome/allrealigned.sorted.bam',
    output:
        gn_peak_dir+'/clam_by_sample/b{bin_size}_p{pvalue_clam}/{sample_id}.bed'
    log: gn_peak_dir+'/clam_by_sample/b{bin_size}_p{pvalue_clam}/log/{sample_id}.log'
    threads: config['threads_mapping']
    params:
        # tmpbed = gn_peak_dir+'/clam_by_sample/b{bin_size}_p{pvalue_clam}/{sample_id}.bed',
        minPeakLength = config['min_peak_size'],
        maxPeakLength = config['max_peak_size'],
        strandness2 = strandness2,
        bin_size = config['bin_size'],
        gtf = config['clam_gtf_gn'],
        out_dir = gn_peak_dir+'/clam_by_sample/b{bin_size}_p{pvalue_clam}/{sample_id}',
        out_file = gn_peak_dir+'/clam_by_sample/b{bin_size}_p{pvalue_clam}/{sample_id}/narrow_peak.permutation.bed',
    # conda:
    #     "./envs/cfpeak.yml"
    shell:
        '''
        mkdir -p {params.out_dir}
        {clam_dir}/CLAM permutation_callpeak \
            -i {input.uniq} {input.realign} \
            -o {params.out_dir} \
            -p {threads} --qval-cutoff {pvalue_clam_param} --gtf {params.gtf} --extend 5 \
            > {log} 2>&1 
        awk '$3-$2>={params.minPeakLength} && $3-$2<={params.maxPeakLength}' {params.out_file} > {output}
        '''
#default: --qval-cutoff 0.005 --merge-size 50 --extend 50
#0.{pvalue}

rule peak_recurrence_clam:
    input:
        bed=lambda wildcards: expand(gn_peak_dir+'/clam_by_sample/b{bin_size}_p{pvalue_clam}/{sample_id}.bed',
            bin_size=bin_size,
            pvalue_clam=pvalue_clam,
            sample_id=sample_ids),
        chrom_sizes=genome_dir+'/chrom_sizes/transcriptome_genome_sort_uniq_newTxID'
    output:
        pos=temp(gn_peak_dir+'/clam_recurrence/b{bin_size}_p{pvalue_clam}.+.bed'),
        neg=temp(gn_peak_dir+'/clam_recurrence/b{bin_size}_p{pvalue_clam}.-.bed'),
        bed=gn_peak_dir+'/clam_recurrence/b{bin_size}_p{pvalue_clam}.bed' # .{strand}
    threads: 2 #config['threads_mapping']
    params: strand = strand
    run:
        shell(peak_recurrence_gn)

rule filter_peak_by_recurrence_clam:
    input:
        gn_peak_dir+'/clam_recurrence/b{bin_size}_p{pvalue_clam}.bed'
    output:
        gn_peak_dir+'/clam/b{bin_size}_p{pvalue_clam}.bed'
    threads: 2 #config['threads_mapping']
    params:
        cov_num=cov_num
    run:
        shell(filter_peak_recurrence)
        

########################################
# run blockbuster
########################################
#peak_dir = config['output_dir']+"/"+config['peak_subdir']
rule call_peaks_blockbuster:
    input:
        # output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/genome/merged.sorted.bam',
        gn_peak_dir+'/gbamStarEM_bed/{sample_id}.genome.bed.gz'
    output:
        inBed=temp(gn_peak_dir+'/gbamStarEM_bed/{sample_id}.reduced.bed'),
        peakBed=gn_peak_dir+'/blockbuster_by_sample/min3/{sample_id}.blockbuster',
        blockBed=gn_peak_dir+'/blockbuster_by_sample/min3/{sample_id}_block.bed',
        clusterBed=gn_peak_dir+'/blockbuster_by_sample/min3/{sample_id}_cluster.bed',
    log: gn_peak_dir+'/blockbuster_by_sample/min3/log/{sample_id}.log'
    threads: 3 #config['threads_mapping']
    params:
        # background=0.99
        tmpbed = gn_peak_dir+'/blockbuster_by_sample/min3/{sample_id}.blockbuster.tmp',
        minPeakLength = config['min_peak_size'],
        maxPeakLength = config['max_peak_size'],
        # strandness2 = strandness2,
        bin_size = config['bin_size'],
        # cutoff = 3,
        minReadNumEachDistinctSite = 3, # default:0 (not filter input reduced read bed)
        blockH = 3, # block: eg.mat_miR (smaller, may overlap with each); default:2
        clusterH = 3, # cluster: eg.pri_miR (larger, locally merged from block, usually not overlapped with each); default:10
    # conda:
    #     "./envs/blockbuster.yml"
    shell:
        r'''
        (echo "start at `date`";
        export PATH=/BioII/lulab_b/baopengfei/mambaforge/envs/blockbuster/bin:$PATH

        python scripts/bamBed2blockbusterBed.py -b {input} -o {output.inBed}
        blockbuster.x -format 1 -minBlockHeight {params.blockH} -minClusterHeight {params.clusterH} -print 1 -tagFilter {params.minReadNumEachDistinctSite} <(cat {output.inBed} | LC_COLLATE=C sort -k1,1 -k2,2n) > {output.peakBed}

        grep -E '>' {output.peakBed} | \
            awk 'BEGIN {{OFS=FS="\t"}} {{ print $2,$3,$4,$1,$6,$5}}' | \
            sed s/'>'/''/g | \
            LC_COLLATE=C sort -k1,1 -k2,2n \
            > {output.clusterBed}
        awk '$3-$2>={params.minPeakLength} && $3-$2<={params.maxPeakLength}' {output.clusterBed} | grep -E "^chr" | grep -vE "Un_|_random|_alt" > {params.tmpbed};
        mv {params.tmpbed} {output.clusterBed}

        grep -E -v '>' {output.peakBed} | \
            awk 'BEGIN {{OFS=FS="\t"}} {{ print $2,$3,$4,$1,$6,$5}}' | \
            LC_COLLATE=C sort -k1,1 -k2,2n \
            > {output.blockBed}
        awk '$3-$2>={params.minPeakLength} && $3-$2<={params.maxPeakLength}' {output.blockBed} | grep -E "^chr" | grep -vE "Un_|_random|_alt" > {params.tmpbed};
        mv {params.tmpbed} {output.blockBed}

        echo "end at `date`" ) > {log} 2>&1
        '''
#awk '$3-$2>=10 && $3-$2<=200' | grep -E "^chr" | grep -vE "Un_|_random|_alt|," | bedtools intersect -s -v -wa -a stdin -b /BioII/lulab_b/baopengfei/shared_reference/hg38/hg38_blacklist.bed > out.bed
#add r''' to avoid shell error (\t to   )

# rule peak_recurrence_blockbuster:
#     input:
#         bed=lambda wildcards: expand(gn_peak_dir+'/blockbuster_by_sample/min3/{sample_id}_cluster.bed',sample_id=sample_ids), # block is overlapped, not for recurr.
#         chrom_sizes=genome_dir+'/chrom_sizes/genome'
#     output:
#         pos=temp(gn_peak_dir+'/blockbuster_recurrence/min3_cluster.+.bed'), # .{strand}
#         neg=temp(gn_peak_dir+'/blockbuster_recurrence/min3_cluster.-.bed'), # .{strand}
#         bed=gn_peak_dir+'/blockbuster_recurrence/min3_cluster.bed', # .{strand}
#     threads: 2 #config['threads_mapping']
#     # params: strand = "+" # strand
#     run:
#         shell(peak_recurrence_gn)

# rule filter_peak_by_recurrence_blockbuster:
#     input:
#         gn_peak_dir+'/blockbuster_recurrence/min3_cluster.bed'
#     output:
#         gn_peak_dir+'/blockbuster/blockbuster_min3_cluster.bed'
#     threads: 2 #config['threads_mapping']
#     params:
#         cov_num=config['cov_num']
#     run:
#         shell(filter_peak_recurrence)


rule peak_consensus_blockbuster:
    input:
        # gn_peak_dir+'/gbamStarEM_bed/{sample_id}.genome.bed.gz'
        lambda wildcards: expand(gn_peak_dir+'/blockbuster_by_sample/min{minReadNumEachDistinctSite}/{sample_id}_block.bed',minReadNumEachDistinctSite=3,sample_id=sample_ids),
    output:
        inBed=temp(gn_peak_dir+'/gbamStarEM_bed/min{minReadNumEachDistinctSite}_reduced.bed'),
        # peakBed=gn_peak_dir+'/blockbuster_by_sample/min3/{sample_id}.blockbuster',
        peakBed=gn_peak_dir+'/blockbuster/blockbuster_min{minReadNumEachDistinctSite}.blockbuster',
        # blockBed=gn_peak_dir+'/blockbuster/blockbuster_min{minReadNumEachDistinctSite}_block.bed',
        # clusterBed=gn_peak_dir+'/blockbuster/blockbuster_min{minReadNumEachDistinctSite}_cluster.bed',
    log: gn_peak_dir+'/blockbuster/log/min{minReadNumEachDistinctSite}.log'
    threads: 3 #config['threads_mapping']
    params:
        # background=0.99
        bed=gn_peak_dir+"/blockbuster_by_sample/min{minReadNumEachDistinctSite}/{"+",".join(sample_ids)+"}_block.bed",
        tmpbed = gn_peak_dir+'/blockbuster/min{minReadNumEachDistinctSite}.bed.tmp',
        # minPeakLength = config['min_peak_size'],
        # maxPeakLength = config['max_peak_size'],
        # strandness2 = strandness2,
        # bin_size = config['bin_size'],
        # cutoff = 3,
        minReadNumEachDistinctSite = 3, # default:0 (not filter input reduced read bed)
        blockH = 3, # block: eg.mat_miR (smaller, may overlap with each); default:2
        clusterH = 3, # cluster: eg.pri_miR (larger, locally merged from block, usually not overlapped with each); default:10
    # conda:
    #     "./envs/blockbuster.yml"
    shell:
        r'''
        (echo "start at `date`";
        export PATH=/BioII/lulab_b/baopengfei/mambaforge/envs/blockbuster/bin:$PATH

        python scripts/bamBed2blockbusterBed.py -b <(cat {params.bed}) -o {output.inBed}
        blockbuster.x -format 1 -minBlockHeight {params.blockH} -minClusterHeight {params.clusterH} -print 1 -tagFilter {params.minReadNumEachDistinctSite} <(cat {output.inBed} | LC_COLLATE=C sort -k1,1 -k2,2n) > {output.peakBed}

        echo "end at `date`" ) > {log} 2>&1
        '''

#blockbuster2gtf
rule blockbuster2gtf:
    input:
        gn_peak_dir+'/blockbuster/blockbuster_min{minReadNumEachDistinctSite}.blockbuster'
    params:
        blacklist = "/BioII/lulab_b/baopengfei/shared_reference/hg38/hg38_blacklist.bed",
        minTxLength = 10,
        maxTxLength = 200
    # conda:
    #     "./envs/blockbuster.yml" # base the same
    output:
        gn_peak_dir+'/blockbuster/blockbuster_min{minReadNumEachDistinctSite}.gtf'
    shell:
        '''
        export PATH=/BioII/lulab_b/baopengfei/mambaforge/envs/blockbuster/bin:$PATH
        python scripts/blockbuster2gtf.py --minTxLength {params.minTxLength} --maxTxLength {params.maxTxLength} -i {input} | bedtools intersect -v -wa -a stdin -b {params.blacklist} > {output}
        '''
        # shell(blockbuster2gtf)

rule gtf2bed:
    input:
        gn_peak_dir+'/blockbuster/blockbuster_min{minReadNumEachDistinctSite}.gtf'
    output:
        block=gn_peak_dir+'/blockbuster/blockbuster_min{minReadNumEachDistinctSite}_block.bed',
        cluster=gn_peak_dir+'/blockbuster/blockbuster_min{minReadNumEachDistinctSite}_cluster.bed'
    shell:
        r'''
        awk 'BEGIN{{FS=OFS="\t"}} 
        {{
            match($9, /transcript_id "([^"]+)"/, arr);
            print $1,$4-1,$5,arr[1],$6,$7
        }}' {input} | LC_COLLATE=C sort -k1,1 -k2,2n | uniq > {output.block}
        
        #need test cluster: Process the input file to keep only one record per unique gene_id
        awk 'BEGIN{FS=OFS="\t"} 
        {
            match($9, /gene_id "([^"]+)"/, arr);
            if (!seen[arr[1]]++) {
                print $1,$4-1,$5,arr[1],$6,$7
            }
        }' {input} | LC_COLLATE=C sort -k1,1 -k2,2n | uniq > {output.cluster}
        '''
        
# #bed2gtf
# bed2gtf="""cat {input} | awk 'BEGIN{{FS=OFS="\t"}} {{print $1,"hg38","exon",($2+1),$3,$5,$6,".","gene_id "$4}}' > {output}"""
# rule bed2gtf:
#     input:
#         gn_peak_dir+'/blockbuster/blockbuster_min3_cluster.bed'
#     output:
#         gn_peak_dir+'/blockbuster/blockbuster_min3_cluster.gtf'
#     run:
#         shell(bed2gtf)

# for STAR with splicing, better use featurecount for read count ? (bed to gtf)
# get count matrix
rule count_matrix_peak:
    input:
        gtf1=gn_peak_dir+'/blockbuster/blockbuster_min{minReadNumEachDistinctSite}.gtf',
        bam=lambda wildcards: expand(output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/genome/{sample_id}.bam',sample_id=sample_ids) # ,rna_type='genome'
        # bam=expand(output_dir+'/gbamStarEM/{sample_id}/'+bam_dedup_dir+'-EM/genome/merged.sorted.bam',sample_id=sample_ids) # ,rna_type='genome'
    output:
        gene_matrix1=gn_peak_dir+'/count_matrix/blockbuster_block_min{minReadNumEachDistinctSite}.txt', # region
        gene_sum1=gn_peak_dir+"/count_matrix/blockbuster_block_min{minReadNumEachDistinctSite}.txt.summary",
        gene_matrix2=gn_peak_dir+'/count_matrix/blockbuster_cluster_min{minReadNumEachDistinctSite}.txt', # region
        gene_sum2=gn_peak_dir+"/count_matrix/blockbuster_cluster_min{minReadNumEachDistinctSite}.txt.summary",
        # gene_matrix=outdir+"/matrix/count_matrix_{region}.txt",
        # gene_sum=outdir+"/matrix/count_matrix_{region}.txt.summary",
        # gene_CPM_TMM=outdir+"/matrix/CPM-TMM_matrix_{region}.txt",
    log:
        log1=gn_peak_dir+"/count_matrix/log/count_matrix_blockbuster_block_min{minReadNumEachDistinctSite}.log",
        log2=gn_peak_dir+"/count_matrix/log/count_matrix_blockbuster_cluster_min{minReadNumEachDistinctSite}.log",
        # CPM_TMM=gn_peak_dir+"/count_matrix/log/CPM-TMM_matrix_{region}.log",
    conda:
        # "envs/DNA.yml"
        "envs/count_diff.yml"
    threads: config['threads_mapping'] #16
    params:
        bam=output_dir+"/gbamStarEM/*/"+bam_dedup_dir+"-EM/genome/{"+",".join(sample_ids)+"}.bam",
        tmp=gn_peak_dir+"/count_matrix/tmp_blockbuster_min{minReadNumEachDistinctSite}",
        # region="{region}",
        # -s: strandness # 1: forward, 2: reverse, 0:unstrand (default)
    shell:
        """
        #--fracOverlap 0.5  # for overlapping features
        featureCounts -T {threads} -O -s 1 -t exon -g transcript_id -M -p \
            -a {input.gtf1} --fracOverlap 0.5 \
            -o {output.gene_matrix1} {params.bam} \
            > {log.log1} 2>&1
        Rscript scripts/multiFeatureCounts2countMat.R \
            -i {output.gene_matrix1}  \
            -o {params.tmp} ;
        mv {params.tmp} {output.gene_matrix1}

        featureCounts -T {threads} -O -s 1 -t exon -g transcript_id -M -p \
            -a {input.gtf1} --fracOverlap 0.5 \
            -o {output.gene_matrix2} {params.bam} \
            > {log.log2} 2>&1
        Rscript scripts/multiFeatureCounts2countMat.R \
            -i {output.gene_matrix2}  \
            -o {params.tmp} ;
        mv {params.tmp} {output.gene_matrix2}
        """
# Rscript scripts/run-NormCountMat.R \
#     -i {output.gene_matrix} \
#     -o {output.gene_CPM_TMM} \
#     -m TMM \
#     > {log.CPM_TMM} 2>&1

# Rscript scripts/run-NormCountMat.R \
#     -i {output.gene_matrix} \
#     -o {output.gene_CPM} \
#     -m none \
#     > {log.CPM} 2>&1
