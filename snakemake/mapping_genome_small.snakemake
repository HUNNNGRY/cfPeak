include: 'common.snakemake'
#include: 'peak_common.snakemake'

from collections import OrderedDict

#data_dir = config['data_dir']
output_dir = config['output_dir']
#rna_types = config['rna_types']
#adaptor = config['adaptor']
#min_read_length = config['min_read_length']
genome_dir = config['genome_dir']
#max_read_length = config['max_read_length']
#min_base_quality = config['min_base_quality']
#temp_dir = config['temp_dir']

def get_all_inputs(wildcards):
    available_inputs = dict(
        #unmapped_other=expand('{output_dir}/unmapped/{sample_id}/other.fa.gz',
        #    output_dir=output_dir, sample_id=sample_ids),
        map_genome=expand('{output_dir}/gbam/{sample_id}/genome.bam', output_dir=output_dir, sample_id=sample_ids),
        
        # tbigwig=expand('{output_dir}/tbigwig/{sample_id}.{rna_type}.bigWig',
        #     output_dir=output_dir, sample_id=sample_ids, rna_type=['transcriptome']),
        # tbigwig_normalized=expand('{output_dir}/tbigwig_normalized/{sample_id}.{rna_type}.bigWig',
        #     output_dir=output_dir, sample_id=sample_ids, rna_type=['transcriptome']),
        gbigwig=expand('{output_dir}/bigwig/{sample_id}.{rna_type}.{strand}.bigWig',
            output_dir=output_dir, sample_id=sample_ids, rna_type=['genome'], strand=['+', '-']),
        gbigwig_normalized=expand('{output_dir}/bigwig_normalized/{sample_id}.{rna_type}.{strand}.bigWig',
            output_dir=output_dir, sample_id=sample_ids, rna_type=['genome'], strand=['+', '-']),
        # gbigwig_normalized_log2=expand('{output_dir}/bigwig_normalized_log2/{sample_id}.{rna_type}.{strand}.bigWig',
        #     output_dir=output_dir, sample_id=sample_ids, rna_type=['other', 'transcriptome'], strand=['+', '-'])
    )
    enabled_inputs = list(available_inputs.keys())
    inputs = []
    for key, l in available_inputs.items():
        if key in enabled_inputs:
            inputs += l
    return inputs

rule all:
    input:
        get_all_inputs

rule mapping_gn_bowtie2:
    input:
        #reads='{output_dir}/unmapped/{sample_id}/clean.fa.gz',
        reads=output_dir + '/trimmed/{sample_id}.fastq.gz',
        index=genome_dir + '/genome_index/bowtie2/genome.1.bt2'
    output:
        unmapped=output_dir + '/unmapped/{sample_id}/genome.fa.gz',
        bam=output_dir + '/gbam/{sample_id}/genome.bam'
	#bai=output_dir + '/gbam/{sample_id}/genome.bam.bai'
    log:
        bam=output_dir + '/gbam/{sample_id}/genome.log'
    params:
        index=genome_dir + '/genome_index/bowtie2/genome'
    shell:
        '''pigz -d -c {input.reads} \
        | bowtie2 -p {threads} --very-fast --no-unal \
            --un-gz {output.unmapped} -x {params.index} - -S - \
        | samtools view -b -o {output.bam} > {log} 2>&1
        '''
#currently no EM, thus rm '-k 100 '
#pigz -d -c {input.reads} \
#        | bowtie2 -p {threads} --sensitive --no-unal \
#            --un-gz {output.unmapped} -x {params.index} - -S - \
#        | samtools view -b -o {output.bam}
#samtools index {output.bam}  
#include: 'sequential_mapping.snakemake'

rule sort_gbam:
    '''Sort genomic BAM file by coordinate
    '''
    input:
        output_dir+'/gbam/{sample_id}/{rna_type}.bam'
    output:
        bam=output_dir+'/gbam_sorted/{sample_id}/{rna_type}.bam',
        bai=output_dir+'/gbam_sorted/{sample_id}/{rna_type}.bam.bai'
    params:
        temp_dir=config['temp_dir']
    shell:
        '''samtools sort -T {params.temp_dir} -o {output.bam} {input}
        samtools index {output.bam}
        '''

rule gbam_to_bedgraph:
    input:
        output_dir+'/gbam_sorted/{sample_id}/{rna_type}.bam'
    output:
        output_dir+'/bedgraph/{sample_id}.{rna_type}.{strand}.bedGraph'
    params:
        temp_dir=config['temp_dir']
    shell:
        '''bedtools genomecov -ibam {input} -strand {wildcards.strand} -bg -split \
            | LC_COLLATE=C sort -T {params.temp_dir} -k1,1 -k2,2n > {output}
        '''

rule gbedgraph_to_bigwig:
    input:
        bedgraph=output_dir+'/bedgraph/{sample_id}.{rna_type}.{strand}.bedGraph',
        chrom_sizes=genome_dir + '/chrom_sizes/genome'
    output:
        output_dir+'/bigwig/{sample_id}.{rna_type}.{strand}.bigWig'
    shell:
        '''{bin_dir}/bedGraphToBigWig {input.bedgraph} {input.chrom_sizes} {output}
        '''

rule read_counts_mapped_gn:
    input: 
        output_dir+'/gbam_sorted/{sample_id}/{rna_type}.bam'
    output:
        output_dir+'/stats/read_counts_mapped/{sample_id}/{rna_type}'
    # conda:
    #     "./envs/cfpeak.yml"
    # wildcard_constraints:
    #     rna_type='(?!promoter$)(?!enhancer$)(?!intron$)(?!repeats$)(?!genome).*'
    shell:
        '''bamtools count -in {input} > {output}
        '''

rule summarize_read_counts_gn:
    input:
        # output_dir+'/stats/read_counts_mapped/{sample_id}/{merge_type}' #merge19_sort merge11RNA_sort
        mapped=lambda wildcards: expand(output_dir+'/stats/read_counts_mapped/{sample_id}/{rna_type}',
            sample_id=sample_ids, rna_type='genome'), # + ['other', 'promoter', 'enhancer', 'intron', 'repeats']),
    output:
        output_dir+'/summary/read_counts.txt'
    run:
        import pandas as pd
        import os
        from collections import OrderedDict
    
        records = OrderedDict()
        for sample_id in sample_ids:
            records[sample_id] = {}
        for filename in input.mapped:
            sample_id, rna_type = filename.split(os.path.sep)[-2:]
            with open(filename, 'r') as f:
                records[sample_id][rna_type + '.mapped'] = int(f.read().strip())
        # for filename in input.unmapped:
        #     sample_id, rna_type = filename.split(os.path.sep)[-2:]
        #     with open(filename, 'r') as f:
        #         records[sample_id][rna_type + '.unmapped'] = int(f.read().strip())
        records = pd.DataFrame.from_records(records)
        records.columns.name = 'sample_id'
        records.columns.name = 'item'
        records.index.name = 'reads_type'
        records.to_csv(output[0], sep='\t', header=True, index=True)

rule normalize_gbigwig:
    input:
        summary=output_dir+'/summary/read_counts.txt',
        # bam='{output_dir}/gbam/{sample_id}/{rna_type}.bam',
        bam=output_dir+'/gbam_sorted/{sample_id}/{rna_type}.bam',
        bigwig=output_dir+'/bigwig/{sample_id}.{rna_type}.{strand}.bigWig',
        chrom_sizes=genome_dir + '/chrom_sizes/genome'
    output:
        bigwig=output_dir+'/bigwig_normalized/{sample_id}.{rna_type}.{strand}.bigWig',
        bedgraph=temp(output_dir+'/bigwig_normalized/{sample_id}.{rna_type}.{strand}.bedGraph')
    wildcard_constraints:
        strand='[+-]'
    run:
        library_size = get_library_size_small(input.summary, wildcards.sample_id, 'genome')
        shell(r'''{bin_dir}/bigWigToBedGraph {input.bigwig} stdout \
            | awk -v d="{library_size}" 'BEGIN{{OFS="\t";FS="\t"}}{{print $1,$2,$3,1000000.0*$4/d}}' > {output.bedgraph}''')
        shell(r'''{bin_dir}/bedGraphToBigWig {output.bedgraph} {input.chrom_sizes} {output.bigwig}''')

# rule normalize_log2_gbigwig:
#     input:
#         bigwig='{output_dir}/bigwig_normalized/{sample_id}.{rna_type}.{strand}.bigWig',
#         chrom_sizes=genome_dir + '/chrom_sizes/genome'
#     output:
#         bigwig='{output_dir}/bigwig_normalized_log2/{sample_id}.{rna_type}.{strand}.bigWig',
#         bedgraph=temp('{output_dir}/bigwig_normalized_log2/{sample_id}.{rna_type}.{strand}.bedGraph')
#     wildcard_constraints:
#         strand='[+-]'
#     shell:
#         r'''bigWigToBedGraph {input.bigwig} stdout \
#             | awk 'BEGIN{{OFS="\t";FS="\t"}}{{print $1,$2,$3,log($4+0.25)/log(2)}}' > {output.bedgraph}
#         bedGraphToBigWig {output.bedgraph} {input.chrom_sizes} {output.bigwig} 
#         '''

