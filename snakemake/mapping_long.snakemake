include: 'common.snakemake'

map_steps = ['rRNA', 'genome', 'circRNA']

rule all:
    input:
        summarize_read_length=expand('{output_dir}/stats/mapped_read_length_by_sample/{sample_id}',
            output_dir=output_dir, sample_id=sample_ids),
        summarize_insert_size=expand('{output_dir}/stats/mapped_insert_size_by_sample/{sample_id}',
            output_dir=output_dir, sample_id=sample_ids),
        summarize_read_counts=expand('{output_dir}/summary/read_counts.txt',
            output_dir=output_dir, sample_id=sample_ids),
        count_matrix=expand('{output_dir}/count_matrix/{count_method}.txt',
            output_dir=output_dir, count_method=config['count_method'])

rule count_clean_reads_pe:
    input:
        '{output_dir}/unmapped/{sample_id}/clean_1.fa.gz'
    output:
        '{output_dir}/stats/read_counts_clean/{sample_id}'
    shell:
        '''gzip -d -c {input} | wc -l | awk '{print int($0/2)}' > {output}
        '''

map_command_pe = '''STAR --genomeDir {params.index} \
            --readFilesIn {input.reads1} {input.reads2} \
            --runThreadN {threads} \
            --outFileNamePrefix {params.output_prefix} \
            --outSAMtype BAM Unsorted \
            --outReadsUnmapped Fastx \
            --readFilesCommand gzip -d -c \
            --outSAMmultNmax 1 \
            --outMultimapperOrder Random
        mv {params.output_prefix}Aligned.out.bam {output.bam}
        gzip -c {params.output_prefix}Unmapped.out.mate1 > {output.unmapped1}
        gzip -c {params.output_prefix}Unmapped.out.mate2 > {output.unmapped2}
        rm -f {params.output_prefix}Unmapped.out.mate1 {params.output_prefix}Unmapped.out.mate2
        '''

rule map_rRNA_paired:
    input:
        reads1='{output_dir}/unmapped/{sample_id}/clean_1.fa.gz',
        reads2='{output_dir}/unmapped/{sample_id}/clean_2.fa.gz',
        index=genome_dir + '/index/star/rRNA/SA'
    output:
        bam='{output_dir}/bam/{sample_id}/rRNA.bam',
        unmapped1='{output_dir}/unmapped/{sample_id}/rRNA_1.fa.gz',
        unmapped2='{output_dir}/unmapped/{sample_id}/rRNA_2.fa.gz',
        log='{output_dir}/mapping_star/{sample_id}/rRNA/Log.final.out'
    params:
        output_prefix='{output_dir}/mapping_star/{sample_id}/rRNA/',
        index=genome_dir + '/index/star/rRNA'
    threads:
        config['threads_mapping']
    run:
        shell(map_command_pe)

rule map_genome_paired:
    input:
        reads1='{output_dir}/unmapped/{sample_id}/rRNA_1.fa.gz',
        reads2='{output_dir}/unmapped/{sample_id}/rRNA_2.fa.gz',
        index=genome_dir + '/long_index/star/SA'
    output:
        bam='{output_dir}/bam/{sample_id}/genome.bam',
        unmapped1='{output_dir}/unmapped/{sample_id}/genome_1.fa.gz',
        unmapped2='{output_dir}/unmapped/{sample_id}/genome_2.fa.gz',
        log='{output_dir}/mapping_star/{sample_id}/genome/Log.final.out'
    params:
        output_prefix='{output_dir}/mapping_star/{sample_id}/genome/',
        index=genome_dir + '/long_index/star'
    threads:
        config['threads_mapping']
    run:
        shell(map_command_pe)
    
rule map_circRNA_paired:
    input:
        reads1='{output_dir}/unmapped/{sample_id}/genome_1.fa.gz',
        reads2='{output_dir}/unmapped/{sample_id}/genome_2.fa.gz',
        index=genome_dir + '/index/star/circRNA/SA'
    output:
        bam='{output_dir}/bam/{sample_id}/circRNA.bam',
        unmapped1='{output_dir}/unmapped/{sample_id}/circRNA_1.fa.gz',
        unmapped2='{output_dir}/unmapped/{sample_id}/circRNA_2.fa.gz',
        log='{output_dir}/mapping_star/{sample_id}/circRNA/Log.final.out'
    params:
        output_prefix='{output_dir}/mapping_star/{sample_id}/circRNA/',
        index=genome_dir + '/index/star/circRNA'
    threads:
        config['threads_mapping']
    run:
        shell(map_command_pe)

rule sort_bam_by_name:
    input:
        '{output_dir}/bam/{sample_id}/genome.bam'
    output:
        '{output_dir}/bam_sorted_by_name/{sample_id}/genome.bam'
    shell:
        '''picard SortSam I={input} O={output} SORT_ORDER=queryname
        '''

rule remove_duplicates:
    input:
        bam='{output_dir}/bam_sorted_by_name/{sample_id}/genome.bam'
    output:
        bam='{output_dir}/bam/{sample_id}/remove_duplicates.bam',
        metrics='{output_dir}/log/remove_duplicates/{sample_id}'
    shell:
        '''picard MarkDuplicates REMOVE_DUPLICATES=true \
            ASSUME_SORT_ORDER=queryname \
            I={input.bam} \
            O={output.bam} \
            M={output.metrics}
        '''

rule count_circRNA:
    input:
        '{output_dir}/bam/{sample_id}/circRNA.bam'
    output:
        '{output_dir}/counts/count_circrna/{sample_id}/circRNA'
    params:
        paired_end={True: '-p', False: ''}[config['paired_end']],
        min_mapping_quality=config['min_mapping_quality']
    shell:
        '''{bin_dir}/count_reads.py count_circrna -q {params.min_mapping_quality} {params.paired_end} -i {input} -o {output}
        '''

rule featurecounts:
    input:
        bam='{output_dir}/bam/{sample_id}/{map_step}.bam',
        gtf=genome_dir + '/gtf/long_RNA.gtf'
    output:
        counts='{output_dir}/counts/featurecounts/{sample_id}/{map_step}',
        summary='{output_dir}/counts/featurecounts/{sample_id}/{map_step}.summary'
    params:
        strandness={'forward': 1, 'reverse': 2}.get(config['strandness'], 0),
        paired_end={True: '-p', False: ''}[config['paired_end']],
        min_mapping_quality=config['min_mapping_quality']
    log:
        '{output_dir}/log/featurecounts/{sample_id}/{map_step}'
    shell:
        '''featureCounts -t exon -g gene_id -M -s {params.strandness} -Q {params.min_mapping_quality} \
            {params.paired_end} -a {input.gtf} -o {output.counts} {input.bam} > {log}
        '''

rule htseq:
    input:
        bam='{output_dir}/bam/{sample_id}/{map_step}.bam',
        gtf=genome_dir + '/gtf/long_RNA.gtf'
    output:
        counts='{output_dir}/counts/htseq/{sample_id}/{map_step}'
    params:
        strandness={'forward': 'yes', 'reverse': 'reverse'}.get(config['strandness'], 'no'),
        min_mapping_quality=config['min_mapping_quality']
    shell:
        '''htseq-count -t exon -i gene_id -f bam -a {params.min_mapping_quality} \
            -s {params.strandness} {input.bam} {input.gtf} > {output.counts}
        '''

rule samtools_stats:
    input:
        '{output_dir}/bam/{sample_id}/{map_step}.bam'
    output:
        '{output_dir}/samtools_stats/{sample_id}/{map_step}.txt'
    shell:
        '''samtools stats {input} > {output}
        '''

rule count_clean_fragments:
    input:
        '{output_dir}/unmapped/{sample_id}/clean_1.fa.gz'
    output:
        '{output_dir}/stats/fragment_counts/{sample_id}/clean'
    threads:
        config['threads_compress']
    shell:
        '''pigz -p {threads} -d -c {input} | wc -l | awk '{{print int($0/2)}}' > {output}
        '''

rule parse_samtools_stats_pe:
    input:
        '{output_dir}/samtools_stats/{sample_id}/{map_step}.txt'
    output:
        fragment_counts='{output_dir}/stats/fragment_counts/{sample_id}/{map_step}',
        insert_size_average='{output_dir}/stats/insert_size_average/{sample_id}/{map_step}',
        insert_size_hist='{output_dir}/stats/insert_size_hist/{sample_id}/{map_step}',
        read_length_hist='{output_dir}/stats/read_length_hist/{sample_id}/{map_step}'
    wildcard_constraints:
        map_step='(?!clean).*'
    shell:
        '''awk 'BEGIN{{OFS="\t";FS="\t"}}/^SN/{{if($2 == "reads mapped and paired:") print int($3/2)}}' {input} > {output.fragment_counts}
        awk 'BEGIN{{OFS="\t";FS="\t"}}/^SN/{{if($2 == "insert size average:") print $3}}' {input} > {output.insert_size_average}
        awk 'BEGIN{{OFS="\t";FS="\t"}}/^IS/{{print $2,$3}}' {input} > {output.insert_size_hist}
        awk 'BEGIN{{OFS="\t";FS="\t"}}/^RL/{{print $2,$3}}' {input} > {output.read_length_hist}
        '''

rule summarize_read_length:
    input:
        lambda wildcards: expand('{output_dir}/stats/read_length_hist/{sample_id}/{map_step}',
            output_dir=wildcards.output_dir, sample_id=wildcards.sample_id, map_step=map_steps)
    output:
        '{output_dir}/stats/mapped_read_length_by_sample/{sample_id}'
    run:
        import pandas as pd

        matrix = {}
        for filename in input:
            map_step = filename.split('/')[-1]
            matrix[map_step] = pd.read_table(filename, sep='\t', index_col=0, header=None, names=['read_length', map_step]).iloc[:, 0]
        matrix = pd.DataFrame(matrix)
        matrix = matrix.loc[:, map_steps]
        matrix.fillna(0, inplace=True)
        matrix = matrix.astype('int')
        matrix.index.name = 'read_length'
        matrix.to_csv(output[0], sep='\t', header=True, index=True)

rule summarize_insert_size:
    input:
        lambda wildcards: expand('{output_dir}/stats/insert_size_hist/{sample_id}/{map_step}',
            output_dir=wildcards.output_dir, sample_id=wildcards.sample_id, map_step=map_steps)
    output:
        '{output_dir}/stats/mapped_insert_size_by_sample/{sample_id}'
    run:
        import pandas as pd

        matrix = {}
        for filename in input:
            map_step = filename.split('/')[-1]
            matrix[map_step] = pd.read_table(filename, sep='\t', index_col=0, header=None, names=['insert_size', map_step]).iloc[:, 0]
        matrix = pd.DataFrame(matrix)
        matrix = matrix.loc[:, map_steps]
        matrix.fillna(0, inplace=True)
        matrix = matrix.astype('int')
        matrix.index.name = 'insert_size'
        matrix.to_csv(output[0], sep='\t', header=True, index=True)

rule summarize_fragment_counts:
    input:
        fragment_counts=lambda wildcards: expand('{output_dir}/stats/fragment_counts/{sample_id}/{map_step}',
            output_dir=wildcards.output_dir, sample_id=sample_ids, map_step=map_steps + ['clean']),
        count_matrix='{{output_dir}}/count_matrix/{count_method}.txt'.format(count_method=config['count_method'])
    output:
        '{output_dir}/summary/read_counts.txt'
    run:
        import pandas as pd
        # read fragment counts for each mapping step
        matrix = pd.DataFrame(index=map_steps + ['clean'], columns=sample_ids)
        for filename in input.fragment_counts:
            c = filename.split('/')
            sample_id = c[-2]
            map_step = c[-1]
            with open(filename, 'r') as f:
                matrix.loc[map_step, sample_id] = int(f.read().strip())
        matrix = matrix.astype('int')
        matrix.columns.name = 'sample_id'
        matrix.index.name = 'rna_type'

        # read count matrix
        count_matrix = pd.read_table(input.count_matrix, sep='\t', index_col=0)
        feature_info = count_matrix.index.to_series().str.split('|', expand=True)
        feature_info.columns = ['gene_id', 'gene_type', 'gene_name', 'domain_id', 'transcript_id', 'start', 'end']
        count_matrix = pd.concat([feature_info, count_matrix], axis=1)
        counts_by_rnatype = count_matrix.groupby('gene_type')[sample_ids].sum()
        counts_by_rnatype = counts_by_rnatype.loc[:, sample_ids]

        matrix = pd.concat([matrix, counts_by_rnatype], axis=0)
        matrix.to_csv(output[0], sep='\t', header=True, index=True, na_rep='NA')

def get_count_matrix_input_counts(wildcards):
    if wildcards.count_method.endswith('_rmdup'):
        template = '{output_dir}/counts/{count_method}/{sample_id}/remove_duplicates'
    else:
        template = '{output_dir}/counts/{count_method}/{sample_id}/genome'
    return expand(template, output_dir=wildcards.output_dir, count_method=wildcards.count_method, sample_id=sample_ids)

rule count_matrix:
    input:
        counts=lambda wildcards: expand('{output_dir}/counts/{count_method}/{sample_id}/genome',
            output_dir=wildcards.output_dir, count_method=wildcards.count_method, sample_id=sample_ids),
        circrna_counts=lambda wildcards: expand('{output_dir}/counts/count_circrna/{sample_id}/circRNA', 
            output_dir=wildcards.output_dir, count_method=wildcards.count_method, sample_id=sample_ids),
        transcript_table=genome_dir + '/transcript_table/all.txt',
        circrna_sizes=genome_dir + '/chrom_sizes/circRNA'
    output:
        '{output_dir}/count_matrix/{count_method}.txt'
    run:
        import pandas as pd
        import re

        count_method = re.sub(r'_rmdup', '', wildcards.count_method)

        # annotate features
        transcript_table = pd.read_table(input.transcript_table, sep='\t', dtype='str')
        transcript_table = transcript_table.drop_duplicates('gene_id', keep='first')
        transcript_table.set_index('gene_id', inplace=True, drop=False)
        # read gene counts from individual files
        matrix = {}
        sample_ids = []
        for filename in input.counts:
            sample_id = filename.split('/')[-2]
            sample_ids.append(sample_id)
            if count_method == 'featurecounts':
                matrix[sample_id] = pd.read_table(filename, comment='#', sep='\t', index_col=0)
                matrix[sample_id] = matrix[sample_id].iloc[:, -1]
            elif count_method == 'htseq':
                matrix[sample_id] = pd.read_table(filename, comment='__', sep='\t', header=None, index_col=0).iloc[:, 0]
        matrix = pd.DataFrame(matrix)
        matrix = matrix.loc[:, sample_ids]
        # remove all-zero features
        matrix = matrix.loc[matrix.sum(axis=1) > 0].copy()
        gene_ids = matrix.index.values
        # read circRNA counts from individual files
        matrix_circrna = {}
        for filename in input.circrna_counts:
            sample_id = filename.split('/')[-2]
            matrix_circrna[sample_id] = pd.read_table(filename, sep='\t', header=None, index_col=0).iloc[:, 0]
        matrix_circrna = pd.DataFrame(matrix_circrna)
        matrix_circrna = matrix_circrna.loc[:, sample_ids]
        matrix_circrna.fillna(0, inplace=True)
        matrix_circrna = matrix_circrna.astype('int')
        matrix_circrna = matrix_circrna.loc[matrix_circrna.sum(axis=1) > 0].copy()
        # annotate circRNA 
        circrna_sizes = pd.read_table(input.circrna_sizes, sep='\t', header=None, index_col=0).iloc[:, 0]
        circrna_ids = matrix_circrna.index.values
        matrix_circrna.index = circrna_ids + '|circRNA|' + circrna_ids + '|' + circrna_ids\
             + '|' + circrna_ids + '|0|' + circrna_sizes.loc[circrna_ids].values.astype('str')
        
        # remove features not in transcript table
        gene_ids = gene_ids[~(transcript_table.loc[gene_ids, 'gene_id'].isna().values)]
        matrix = matrix.loc[gene_ids]
        # annotate features
        feature_names = transcript_table.loc[gene_ids, 'gene_id'].values \
            + '|' + transcript_table.loc[gene_ids, 'gene_type'].values \
            + '|' + transcript_table.loc[gene_ids, 'gene_name'].values \
            + '|' + transcript_table.loc[gene_ids, 'gene_id'].values \
            + '|' + transcript_table.loc[gene_ids, 'chrom'].values \
            + '|' + transcript_table.loc[gene_ids, 'start'].values \
            + '|' + transcript_table.loc[gene_ids, 'end'].values
        
        matrix.index = feature_names
        # merge gene matrix and circRNA matrix
        matrix = pd.concat([matrix, matrix_circrna], axis=0)
        matrix.index.name = 'feature'
        
        matrix.to_csv(output[0], sep='\t', header=True, index=True, na_rep='NA')
