include: 'common.snakemake'

from collections import OrderedDict

data_dir = config['data_dir']
output_dir = config['output_dir']
rna_types = config['rna_types']
adaptor = config['adaptor']
min_read_length = config['min_read_length']
genome_dir = config['genome_dir']
max_read_length = config['max_read_length']
min_base_quality = config['min_base_quality']
temp_dir = config['temp_dir']

def get_all_inputs(wildcards):
    available_inputs = dict(
        summarize_read_counts=expand('{output_dir}/summary/read_counts.txt',
            output_dir=output_dir),
        mapped_read_length=expand('{output_dir}/stats/mapped_read_length_by_sample/{sample_id}',
            output_dir=output_dir, sample_id=sample_ids),
        summarize_read_counts_jupyter=expand('{output_dir}/summary/read_counts.html',
            output_dir=output_dir),
        unmapped_other=expand('{output_dir}/unmapped/{sample_id}/other.fa.gz',
            output_dir=output_dir, sample_id=sample_ids),
        map_genome=expand('{output_dir}/gbam/{sample_id}/genome.bam',
            output_dir=output_dir, sample_id=sample_ids)
    )
    enabled_inputs = list(available_inputs.keys())
    inputs = []
    for key, l in available_inputs.items():
        if key in enabled_inputs:
            inputs += l
    return inputs

rule all:
    input:
        get_all_inputs

# rule read_counts_mapped:
#     input:
#         lambda wildcards: '{output_dir}/{bam_type}/{sample_id}/{rna_type}.bam'.format(
#             output_dir=wildcards.output_dir, 
#             bam_type='gbam' if wildcards.rna_type == 'other' else 'tbam',
#             sample_id=wildcards.sample_id, rna_type=wildcards.rna_type
#         )
#     output:
#         '{output_dir}/stats/read_counts_mapped/{sample_id}/{rna_type}'
#     wildcard_constraints:
#         rna_type='(?!promoter$)(?!enhancer$)(?!intron$)(?!repeats$)(?!genome).*'
#     shell:
#         '''bamtools count -in {input} > {output}
#         '''

# rule summarize_read_counts:
#     input:
#         mapped=lambda wildcards: expand('{output_dir}/stats/read_counts_mapped/{sample_id}/{rna_type}',
#             output_dir=wildcards.output_dir, sample_id=sample_ids, 
#             rna_type=rna_types + ['other', 'promoter', 'enhancer', 'intron', 'repeats']),
#         unmapped=lambda wildcards: expand('{output_dir}/stats/read_counts_unmapped/{sample_id}/{rna_type}',
#             output_dir=wildcards.output_dir, sample_id=sample_ids, rna_type=['clean', 'other'])
#     output:
#         '{output_dir}/summary/read_counts.txt'
#     run:
#         import pandas as pd
#         import os
#         from collections import OrderedDict
    
#         records = OrderedDict()
#         for sample_id in sample_ids:
#             records[sample_id] = {}
#         for filename in input.mapped:
#             sample_id, rna_type = filename.split(os.path.sep)[-2:]
#             with open(filename, 'r') as f:
#                 records[sample_id][rna_type + '.mapped'] = int(f.read().strip())
#         for filename in input.unmapped:
#             sample_id, rna_type = filename.split(os.path.sep)[-2:]
#             with open(filename, 'r') as f:
#                 records[sample_id][rna_type + '.unmapped'] = int(f.read().strip())
#         records = pd.DataFrame.from_records(records)
#         records.columns.name = 'sample_id'
#         records.columns.name = 'item'
#         records.index.name = 'reads_type'
#         records.to_csv(output[0], sep='\t', header=True, index=True)

# rule summarize_read_counts_jupyter:
#     input:
#         read_counts='{output_dir}/summary/read_counts.txt',
#         jupyter='templates/summarize_read_counts_small.ipynb'
#     output:
#         jupyter='{output_dir}/summary/read_counts.ipynb',
#         html='{output_dir}/summary/read_counts.html'
#     shell:
#         '''cp {input.jupyter} {output.jupyter}
#         jupyter nbconvert --execute --to html \
#             --HTMLExporter.exclude_code_cell=False \
#             --HTMLExporter.exclude_input_prompt=True \
#             --HTMLExporter.exclude_output_prompt=True \
#             {output.jupyter}
#         '''

# rule mapped_read_length:
#     input:
#         '{output_dir}/tbam/{sample_id}/{rna_type}.bam'
#     output:
#         '{output_dir}/stats/mapped_read_length/{sample_id}/{rna_type}'
#     shell:
#         '''{bin_dir}/statistics.py read_length_hist --max-length 600 -i {input} -o {output}
#         '''

# rule merge_mapped_read_length:
#     input:
#         lambda wildcards: expand('{output_dir}/stats/mapped_read_length/{sample_id}/{rna_type}',
#             output_dir=wildcards.output_dir, sample_id=wildcards.sample_id, rna_type=rna_types)
#     output:
#         '{output_dir}/stats/mapped_read_length_by_sample/{sample_id}'
#     run:
#         import pandas as pd
#         from collections import OrderedDict

#         table = OrderedDict()
#         for filename in input:
#             rna_type = os.path.basename(filename)
#             df = pd.read_table(filename, index_col=0)
#             table[rna_type] = df['query']
#         table = pd.DataFrame(table)
#         table.to_csv(output[0], sep='\t', header=True, index=True)

rule bam_to_bed:
    input:
        bam='{output_dir}/gbam/{sample_id}/{rna_type}.bam'
    output:
        bed='{output_dir}/gbed/{sample_id}/{rna_type}.bed.gz'
    shell:
        '''bedtools bamtobed -i {input} | bedtools sort | pigz -c > {output}
        '''

rule map_genome:
    input:
        reads='{output_dir}/unmapped/{sample_id}/clean.fa.gz',
        index=genome_dir + '/genome_index/bowtie2/genome.1.bt2'
    output:
        unmapped='{output_dir}/unmapped/{sample_id}/genome.fa.gz',
        bam='{output_dir}/gbam/{sample_id}/genome.bam'
    params:
        index=genome_dir + '/genome_index/bowtie2/genome'
    shell:
        '''pigz -d -c {input.reads} \
        | bowtie2 -f -p {threads} --sensitive --no-unal \
            --un-gz {output.unmapped} -x {params.index} - -S - \
        | samtools view -b -o {output.bam}
        '''
    
include: 'sequential_mapping.snakemake'

